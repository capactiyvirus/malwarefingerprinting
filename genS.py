import gensim
import numpy as np
from gensim import corpora
from pprint import pprint
from gensim import models
from gensim.utils import simple_preprocess
from smart_open import smart_open
import os
import nltk
#nltk.download('stopwords')  # run once
#from nltk.corpus import stopwords
#stop_words = stopwords.words('english')

rootDir = '/mnt/c/Documents and Settings/imasi/Desktop/Testingfor 4480/wordlists/00cfd8d2d2f88e85a8a25abdf41adb16wordlist.txt'
ddir = '/mnt/c/Documents and Settings/imasi/Desktop/Testingfor 4480/testdir/'






class BoWCorpus(object):
    def __init__(self, path, dictionary):
        self.filepath = path
        self.dictionary = dictionary

    def __iter__(self):
        global mydict  # OPTIONAL, only if updating the source dictionary.
        for line in smart_open(self.filepath, encoding='latin'):
            # tokenize
            tokenized_list = simple_preprocess(line, deacc=True)

            # create bag of words
            bow = self.dictionary.doc2bow(tokenized_list, allow_update=True)

            # update the source dictionary (OPTIONAL)
            mydict.merge_with(self.dictionary)

            # lazy return the BoW
            yield bow




# Create the Dictionary
mydict = corpora.Dictionary()


# Create the Corpus
corpus = BoWCorpus(rootDir, dictionary=mydict)  # memory friendly
for doc in corpus:
    print(len(doc))
#     print([[mydict[id], freq] for id, freq in doc])


# Print the token_id and count for each line.
#print(len(corpus))

tfidf = models.TfidfModel(corpus, smartirs='ntc')




for doc in tfidf[corpus]:
    print(len(doc))
#     print([[mydict[id], np.around(freq, decimals=2)] for id, freq in doc])




 