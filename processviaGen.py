import gensim
from gensim import models
from gensim import corpora
from pprint import pprint
from gensim.utils import simple_preprocess
from smart_open import smart_open
import os
import numpy as np
from nltk.corpus import stopwords
import re
import logging
import gensim.downloader as api
from gensim.utils import simple_preprocess, lemmatize
from gensim.models import LdaModel, LdaMulticore
import pandas as pd

import matplotlib.pyplot as plt
from gensim.models.coherencemodel import CoherenceModel
from collections import Counter


# Plotting tools 
import pyLDAvis
import pyLDAvis.gensim  # don't skip this
import matplotlib.pyplot as plt
#%matplotlib inline


# spacy for lemmatization
import spacy


logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')
logging.root.setLevel(level=logging.INFO)
stop_words = stopwords.words('english')
stop_words = stop_words + ['com', 'edu', 'subject', 'lines', 'organization', 'would', 'article', 'could']

#dataset = api.load("text8")
#dataset = [wd for wd in dataset]
#print(dataset)


#print(dataset)




#dct = corpora.Dictionary(dataset)
#corpus = [dct.doc2bow(line) for line in dataset]


#bigram = gensim.models.phrases.Phrases(dataset, min_count=3, threshold=10)
#trigram = gensim.models.phrases.Phrases(bigram[dataset], threshold=10)
#print(trigram[bigram[dataset[0]]])
#print(bigram[dataset[0]])


path_to_text_directory = "/mnt/c/Documents and Settings/imasi/Desktop/Testingfor 4480/wordlists/"
alt_path_to_text_directory = "/mnt/c/Documents and Settings/imasi/Desktop/Testingfor 4480/testdir/"


holder = []

class BoWCorpus(object):
    def __init__(self, path, dictionary):
        self.filepath = path
        self.dictionary = dictionary

class ReadTxtFiles(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname), encoding='latin'):
                #print(line.split())
                holder.append(line.split())
                yield simple_preprocess(line)

def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations

def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]

def make_trigrams(texts):
    return [trigram_mod[bigram_mod[doc]] for doc in texts]


# def predictTopics(corpus):
#     #optimal_model = None
#     caption_topics_df = pd.DataFrame()
#     for i, row in enumerate(optimal_model[corpus]):
#         row = sorted(row,key=lambda x: (x[1]),reverse=True)
#         for j, (topic_num, prop_topic) in enumerate(row):
#             if j == 0:
#                 wp = optimal_model.show_topic(topic_num)
#                 topic_keywords = ", ".join([word for word, prop in wp])
#                 caption_topics_df=caption_topics_df.append(pd.Series([int(topic_num),round(prop_topic,4), topic_keywords]),ignore_index=True)
#             else:
#                 break
#     caption_topics_df.columns=['Dominant_Topic','Perc_Contribution','Topic_Keywords']
#     return caption_topics_df

def dominant_topics(ldamodel, corpus, texts):
    sent_topics_df = pd.DataFrame()
    for i, row in enumerate(ldamodel[corpus]):
        c = Counter(x[1] for x in row)
        row = sorted( key=lambda x: (x[1]), reverse=True)
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0: # => dominant topic
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = ", ".join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)
            else:   
                break
    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']
    contents = pd.Series(texts)
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    return(sent_topics_df)







def runLDA():
    
    
    optimal_model = None

    #mydict = corpora.Dictionary(data_words_trigrams)
    
    # for doc in corpus:
    #     print([[mydict[id], freq] for id, freq in doc])




    #tfidf = models.TfidfModel(corpus, smartirs='ntc')
    #tfidf.save("tfidf_model.model")

    START = 32
    END = 34
    STEP = 2

    topic_range = range(START,END,STEP)

    coherence_values = []
    model_list = []

    for num_topics in topic_range:
        lda_model = LdaMulticore(corpus=corpus,
                            id2word=mydict,
                            random_state=100,
                            num_topics=8,
                            passes=1,
                            chunksize=10000,
                            batch=False,
                            alpha='asymmetric',
                            decay=0.5,
                            
                            offset=64,
                            eta=None,
                            eval_every=0,
                            iterations=100,
                            gamma_threshold=0.001,
                            per_word_topics=True)
        model_list.append(lda_model)
        coherencemodel = CoherenceModel(model=lda_model,texts=data_words_trigrams,dictionary=mydict, coherence="c_v")
        coherence_values.append(coherencemodel.get_coherence())
        print('\nCoherence Score: ', coherencemodel.get_coherence())
    max_coherence_val = 0
    for i, (m,cv) in enumerate(zip(topic_range,coherence_values)):
        if max_coherence_val<round(cv,4):
            optimal_model=model_list[i]
            optimal_num_topics = m
            max_coherence_val=round(cv,4)
        print("Num Topics =",m ," has Coherence Value of",round(cv,4))
    
    plt.plot(coherence_values)
    plt.xlabel("Number of Topics")
    plt.ylabel("Coherence score")
    plt.legend(("coherence_values"), loc="best")
    plt.show()

    pyLDAvis.enable_notebook()
    vis = pyLDAvis.gensim.prepare(lda_model, corpus, mydict)
    pyLDAvis.save_html(vis, 'LDA_Visualization.html')


mydict = corpora.Dictionary(ReadTxtFiles(path_to_text_directory))
data = [wd for wd in holder]
newdata = list(sent_to_words(data))


bigram = gensim.models.Phrases(newdata, min_count=5, threshold=100) # higher threshold fewer phrases.
trigram = gensim.models.Phrases(bigram[newdata], threshold=100)  
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)


data_words_bigrams = make_bigrams(newdata)
data_words_trigrams = make_trigrams(data_words_bigrams)
mydict = corpora.Dictionary(data_words_trigrams)
corpus = [mydict.doc2bow(line) for line in data_words_trigrams]
lda_model = LdaModel.load('lda_model.model')




optimal_model = lda_model


df_topic_sents_keywords = dominant_topics(ldamodel=optimal_model, corpus=corpus, texts=data_words_trigrams)
df_dominant_topic = df_topic_sents_keywords.reset_index()

df_dominant_topic.columns = [
   'Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text'
]

df_dominant_topic.head(15)
# df = predictTopics([mydict.doc2bow(text) for text in data_words_trigrams])
# train_df = pd.concat([train_df.reset_index(drop=True),df],axis=1)


# mydict = corpora.Dictionary(ReadTxtFiles(alt_path_to_text_directory))
# data = [wd for wd in holder]

# df = predictTopics([mydict.doc2bow(text) for text in data])
# valid_df = pd.concat([valid_df.reset_index(drop=True),df],axis=1)

    #print('\nPerplexity: ', optimal_model.log_perplexity(corpus))



#lda_model.save('lda_model.model')
#lda_model.print_topics(-1)





#print("START LDA")

# lda_model = LdaMulticore(corpus=corpus,
#                          id2word=mydict,
#                          random_state=100,
#                          num_topics=50,
#                          passes=10,
#                          chunksize=10000,
#                          batch=False,
#                          alpha='asymmetric',
#                          decay=0.5,
#                          workers=8,
#                          offset=64,
#                          eta=None,
#                          eval_every=0,
#                          iterations=1000,
#                          gamma_threshold=0.001,
#                          per_word_topics=True)

#lda_model.save('lda_model.model')
#lda_model.print_topics(-1)

# coherence_model_lda = CoherenceModel(model=lda_model, texts=corpus, dictionary=mydict, coherence='c_v')
# coherence_lda = coherence_model_lda.get_coherence()
# print('\nCoherence Score: ', coherence_lda)
# pyLDAvis.enable_notebook()
# vis = pyLDAvis.gensim.prepare(lda_model, corpus, mydict)
# pyLDAvis.save_html(vis, 'LDA_Visualization.html')

# for i, doc in enumerate(data[:100]):
#     doc_out = []
#     for wd in doc:
#             if wd not in stop_words:
#                 lemmatized_word = lemmatize(wd,allowed_tags=re.compile('(NN|JJ|RB)'))
#                 if lemmatized_word:
#                     doc_out = doc_out + [lemmatized_word[0].split(b'/')][0].decode('utf-8')
#                 else:
#                     continue
#     data_processed.append(doc_out)

# print(data_processed[0][:5]) 

#tfidf = models.TfidfModel(corpus, smartirs='ntc')


# for doc in tfidf[corpus]:
#     print([[mydict[id], np.around(freq, decimals=4)] for id, freq in doc])
