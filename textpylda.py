import numpy as np
import pandas as pd
import lda 
import lda.datasets
import textmining
import os

rootDir = '/mnt/c/Documents and Settings/imasi/Desktop/Testingfor 4480/wordlists/'



d12 = '/mnt/c/Documents and Settings/imasi/Desktop/Testingfor 4480/name.txt'

tdm = textmining.TermDocumentMatrix()
holder = []   
rowHolder = []
totalVocab = []

with open(d12,'r',errors='ignore') as wl:
    title = wl.read()
tdm.add_doc(title)
titleArr = title.split()


for dirName, subdirList, fileList in os.walk(rootDir, topdown=False):
    print('Found directory: %s' % dirName)
    for fname in fileList:
        


        completeName = os.path.join(rootDir, fname)
        with open(completeName,'r',errors='ignore') as wl:
            vocab = wl.read()
        tdm.add_doc(vocab)
        vocabArr = vocab.split()
        totalVocab = vocabArr+totalVocab
        holder.append(vocabArr)


    #print(i+1)
print(len(totalVocab))
holder.append(titleArr)

count = 0
for row in tdm.rows(cutoff=1):
    if count != 0:
        rowHolder.append(row)
    count+=1
print(len(holder))

X = np.array(rowHolder)

print(X)
print(X.shape)
print(X.sum())

model = lda.LDA(n_topics=32, n_iter=2000, random_state=1)
model.fit(X)
topic_word = model.topic_word_
n_top_words = 10




for i, topic_dist in enumerate(topic_word):
    #print(topic_dist)
    topic_words = np.array(totalVocab)[np.argsort(topic_dist)][:-n_top_words:-1]
    print('Topic {}: {}'.format(i, ' '.join(topic_words)))


doc_topic = model.doc_topic_


for i in range(32):
    print("{} (top topic: {})".format(titleArr[i], doc_topic[i].argmax()))























#for i in range(len(rowHolder)):
    #print(i+1)
# START CODE
# tdm = textmining.TermDocumentMatrix()

# tdm.add_doc(vocab)
# tdm.add_doc(title)


# vocabArr = vocab.split()
# titleArr = title.split()
 
# vocabArrnp = np.array(vocabArr)
# titleArrnp = np.array(titleArr)



# holder = []


# count = 0
# for row in tdm.rows(cutoff=1):
#     if count != 0:
#         holder.append(row)
        
#     count+=1

# print(len(holder[0]))

# arr1 = np.array(holder)
# x1 = arr1.shape
# print(arr1.sum())

# model = lda.LDA(n_topics=20, n_iter=1500, random_state=1)
# model.fit(arr1)
# topic_word = model.topic_word_
# n_top_words = 8

# doc_topic = model.doc_topic_

# print(doc_topic)

# for i in range(2):
#     print("{} (top topic: {})".format(titleArr[i], doc_topic[i].argmax()))



# for i, topic_dist in enumerate(topic_word):
#     topic_words = np.array(vocabArr)[np.argsort(topic_dist)][:-n_top_words:-1]
#     print('Topic {}: {}'.format(i, ' '.join(topic_words)))


#END CODE

















#print(holder)
#print
# X = lda.datasets.load_reuters() 
# count = 0
# for rows in X:
#     print(rows)
#     count+=1
#     print(count)
    

#matrix.shape
#print(X)

#vocab = lda.datasets.load_reuters_vocab()
#titles = lda.datasets.load_reuters_titles()

#model = lda.LDA(n_topics=20, n_iter=1500, random_state=1)
#model.fit(tdm)

