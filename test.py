import gensim
from gensim import models
from gensim import corpora
from pprint import pprint
from gensim.utils import simple_preprocess
from smart_open import smart_open
import os
import numpy as np
from nltk.corpus import stopwords
import re
import logging
import gensim.downloader as api
from gensim.utils import simple_preprocess, lemmatize
from gensim.models import LdaModel, LdaMulticore
import pandas as pd
import pickle
import matplotlib.pyplot as plt
from gensim.models.coherencemodel import CoherenceModel
from collections import Counter
from gensim.models.wrappers import LdaMallet
import csv


# Plotting tools 
import pyLDAvis
import pyLDAvis.gensim  # don't skip this
import matplotlib.pyplot as plt
#%matplotlib inline


# spacy for lemmatization
import spacy

#os.environ.update({'MALLET_HOME':'/mnt/c/mallet/'})
#mallet_path = '/mnt/c/mallet/bin/mallet.bat' 


logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')
logging.root.setLevel(level=logging.INFO)
#stop_words = stopwords.words('english')
#stop_words = stop_words + ['graphic','array','iiljava','throwable','ad','assetmanager','leave','display','math','io','load','field','declare','ilandroid','util','style','ibinder','declare','bljava','apache','motionevent','ljava', 'lang', 'string', 'method', 'landroid', 'app','download', 'public', 'url', 'int', 'constructor', 'return', 'void','invoke','direct','private','init','stringbuilder','content','bundle','arraylist','inputstream','stringbuilder','exception','context','class','button','text','charsequence','map','widget','uri','integer','system','runnable','object','activity','ioexception','textview','android','readline','iterator','onclicklistener','log','result','thread','delete','toast','componentname','hashmap','os','declare','outputstream','pendingintent','bufferedreader','telephony','reader','net','iljava','resource','progressbar','intent','message','menu','asynctask','pm','set','alertdialog','build','long','rect','database','keyevent','impl','color','base','application','file','service','reflect','boolean','float','annotation','dialog','date','progressdialog','iilandroid','urlconnection','fail','calendar','buildconfig','handler','menuitem','list','click','select','timer']
stop_words = ['raw','Op','Laf','7','xml','w']

nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])
holder = []
oholder = []
#FINALTESTOFWORDS
path_to_text_directory = "/mnt/c/Documents and Settings/imasi/Desktop/Testingfor 4480/WordLists1/"
text_path = "/mnt/c/Documents and Settings/imasi/Desktop/Testingfor 4480/testdir/"
class ReadTxtFiles(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        counter = 0
        with open("/mnt/g/4480/maldozer_metadata.csv") as csv_file:
            csv_reader = csv.reader(csv_file, delimiter=',')
            for row in csv_reader:
                if(row[0] == "md5"):
                    continue
                if(row[7]!="32"):
                        print(row[7])
                        #print(row[0])
                for fname in os.listdir(self.dirname):
                    #print(row[0] == fname[:-12])
                    
                    if(row[0] == fname[:-12]):
                        
                        counter+=1
                       # print(counter)
                        oholder.append(fname[:-12]+"-"+row[3])
                        for line in open(os.path.join(self.dirname, fname), encoding='latin'):
                            #print(line.split())
                            holder.append(line.split())
                            yield simple_preprocess(line)
                    #else:
                        #print(" IT FAILED @@@@ \n")
                        

def make_bigrams(texts):
   return [bigram_mod[doc] for doc in texts]
def make_trigrams(texts):
   return [trigram_mod[bigram_mod[doc]] for doc in texts]

def sent_to_words(sentences):
   for sentence in sentences:
      yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))


# def coherence_values_computation(dictionary, corpus, texts, limit, start=2, step=3):
#     coherence_values = []
#     model_list = []
#     for num_topics in range(start, limit, step):
#         model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)
#         model_list.append(model)
#     coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
#     coherence_values.append(coherencemodel.get_coherence())
#     return model_list, coherence_values




typeholder = []


def getType():
    with open("/mnt/g/4480/maldozer_metadata.csv") as csv_file:
        csv_reader = csv.reader(csv_file, delimiter=',')
        line_count = 0

        print(len(oholder))
        #print(len(csv_reader))
        for row in csv_reader:
            for name in oholder: 
                if(str(row[0]) == str(name)):
                    
                    print(row[0]+" "+row[3])
                    typeholder.append(row[3])
                    break
        
       
                # if line_count > 0:
                #     print(name)
                #     print(str({row[0]})
                #     if name == {row[0]}:
                #         typeholder.append({row[3]})
                #     #print(f'Column names are {", ".join(row)}')
                # line_count += 1
                
                    #print(f'\t{row[3]}')
                    #line_count += 1
            #print(f'Processed {line_count} lines.')



def topicReport():
    df = pd.DataFrame()


def dominant_topics(ldamode1, corpuss, texts1):
    sent_topics_df = pd.DataFrame()
    #print(sent_topics_df)
    for i, row in enumerate(ldamode1[corpuss]):
        #print(i)

        row = sorted(row[0], key=lambda x: x[1], reverse=True)
        
        #row = sorted(row, key=lambda x: x if len(x)>0 else None, reverse=True)
        for j, (topic_num, prop_topic) in enumerate(row):
            #print(topic_num)
            #print(prop_topic)
            if j == 0: # => dominant topic
                
                wp = ldamode1.show_topic(topic_num)
                
                topic_keywords = ", ".join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords, texts1[i], oholder[i]]), ignore_index=True)
            else:
                break
    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords', 'Texts','Name']
    #contents = pd.Series(texts1)
    #nameContents = pd.Series(oholder)
    #typeholder.reverse()
    #typeVal = pd.Series(typeholder)
    #print(nameContents)
    #sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    #add maybe
    #sent_topics_df = pd.concat([sent_topics_df, nameContents], axis=1)




    #sent_topics_df = pd.concat([sent_topics_df, typeVal], axis=1)
    return(sent_topics_df)




def remove_stopwords(texts):
   return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]

def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
   texts_out = []
   for sent in texts:
     doc = nlp(" ".join(sent))
     texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
   return texts_out


# plaintext = corpora.Dictionary(ReadTxtFiles(path_to_text_directory))
# data1 = [wd for wd in oholder]
# text_words = list(sent_to_words(data1))



########### MAKE DIC AND CORP
info = corpora.Dictionary(ReadTxtFiles(path_to_text_directory))
#print(oholder)
data = [wd for wd in holder]

#data_words = list(sent_to_words(data))

#print(data_words)
data_words = list(data)
#print(data_words)

#print(oholder)

dataText = pd.Series(data_words)
#getType()
#print(typeholder)

#typeholder.reverse()
#print(typeholder)

#data_words = remove_stopwords(data_words)
#print(len(data_words[0]))

##### SAVE ARRAY
#np.savetxt("array.txt", np.array(oholder), fmt="%s")



bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)
trigram = gensim.models.Phrases(bigram[data_words], threshold=100)
bigram_mod = gensim.models.phrases.Phraser(bigram)
trigram_mod = gensim.models.phrases.Phraser(trigram)
#data_words_bigrams = make_bigrams(data_words)
#data_words_trigrams = make_trigrams(data_words_bigrams)


# data_lemmatized = lemmatization(data_words_trigrams ,allowed_postags=[
#    'NOUN', 'ADJ', 'VERB', 'ADV'
# ])


#data_words_trigrams = make_trigrams(data_words_bigrams)










id2word = corpora.Dictionary(data_words)
#id2word.filter_extremes(no_below=3)
#id2word.filter_extremes( no_above=0.5)
corpus = [id2word.doc2bow(text) for text in data_words]


tfidf = models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]

texts = data_words






#rint(texts[0])
# tfidf_weights = {id2word.get(id): value
#                      for doc in corpus_tfidf
#                      for id, value in doc}
# sorted_tfidf_weights = sorted(tfidf_weights.items(), key=lambda w: w[1])

#print(len(sorted_tfidf_weights))
#print(len(corpus_tfidf))

#rare_terms = sorted_tfidf_weights[-7500:]
#print(rare_terms)
#corpus_tfidf = rare_terms

# print(corpus_tfidf)
# for doc in corpus_tfidf:
#     pprint(doc)
#     break

#texts = data_words



######## save DIC AND CORP
id2word.save("MODEL_W_ON_W1/Dictionary")
corpora.MmCorpus.serialize("MODEL_W_ON_W1/Corpus", corpus_tfidf)


#### LOAD DIC AND CORP
#id2word = corpora.Dictionary.load("DIC")
#corpus = corpora.MmCorpus("corpus")


topic_range = range(8,9,1)
coherence_values = []
model_list = []

# for num_topics in topic_range:
#     lda_model = LdaMulticore(
#                             corpus=corpus_tfidf,
#                             id2word=id2word,
#                             random_state=100,
#                             num_topics=11,
#                             passes=1,
#                             chunksize=10000,
#                             #batch=False,
#                             #alpha='asymmetric',
#                             alpha='symmetric',
#                             decay=0.5,
#                             offset=64,
#                             eta=None,
#                             eval_every=0,
#                             iterations=100,
#                             gamma_threshold=0.001,
#                             per_word_topics=True
#     )
#     model_list.append(lda_model)

#     coherencemodel = CoherenceModel(model=lda_model,texts=texts,dictionary=id2word, coherence="c_v")
#     coherence_values.append(coherencemodel.get_coherence())
#     print(coherence_values)


# max_coherence_val = 0

# for i, (m,cv) in enumerate(zip(topic_range,coherence_values)):
#     if max_coherence_val<round(cv,4):
#         optimal_model=model_list[i]
#         optimal_num_topics = m
#         max_coherence_val=round(cv,4)
#     print("Num Topics =",m ," has Coherence Value of",round(cv,4))


# vis = pyLDAvis.gensim.prepare(lda_model, corpus_tfidf, id2word)


# pyLDAvis.save_html(vis, 'MODEL_W_ON_W1/LDA_Visualization.html')
# lda_model.save("MODEL_W_ON_W1/lda_model.model")



########## LOAD LDA MODEL
lda_model = LdaMulticore.load('Result2/lda_model.model')



# vis = pyLDAvis.gensim.prepare(lda_model, corpus_tfidf, id2word)
# pyLDAvis.save_html(vis, 'MODEL_W_ON_W1/LDA_Visualization.html')
# lda_model.save("MODEL_W_ON_W1/lda_model.model")

# for i in range(0,lda_model.num_topics):
#     print(lda_model.show_topic(i))

# ########## TOPIC TO SENTENCES
df_topic_sents_keywords = dominant_topics(
  lda_model, corpus_tfidf, texts
)


df_dominant_topic = df_topic_sents_keywords.reset_index()
df_dominant_topic.columns = [
   'Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text', 'Name'
]
#df_dominant_topic.head(15)
#df_dominant_topic.save("savefile")
# print(lda_model.show_topic(6))
# print(lda_model.show_topic(15))

# #print(data_words[0])
df_dominant_topic.to_html("MODEL_W_ON_W1/DataFrame.html")

print(df_dominant_topic)






# #for x in corpus_tfidf:
# unsee = corpus_tfidf[0]
# vector = lda_model[unsee]
# print(vector[0])



# test_vecs = []
# for i in range(len(texts)):
#     top_topics = lda_model.get_document_topics(corpus_tfidf[i], minimum_probability=None)

#     topic_vec = [top_topics[i][1] for i in range(17)]
#     #topic_vec.extend([texts.iloc[i].real_counts]) # counts of reviews for restaurant
#     #topic_vec.extend([len(texts.iloc[i].text)]) # length review
#     test_vecs.append(topic_vec)

# print(topic_vec)

# print(test_vecs)

















# MALLET STUFF DOESNT WORK RN
# ldamallet = LdaMallet(
#    mallet_path=mallet_path, corpus=corpus, num_topics=5, id2word=id2word
# )
# pprint(ldamallet.show_topics(formatted=False))
# coherencemodel = CoherenceModel(model=ldamallet,texts=texts,dictionary=id2word, coherence="c_v")
# coherence_values.append(coherencemodel.get_coherence())